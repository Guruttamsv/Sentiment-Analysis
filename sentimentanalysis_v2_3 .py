# -*- coding: utf-8 -*-
"""SentimentAnalysis_V2_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Iagm7ZoZxesdJ-4bcKepbF97OaunS8Ch

# Get Data and import libraries that we need
"""

# Define where we will download the data
path_data = "/content/sample_data"

"""Download the data"""

!wget -P /content/sample_data/ -c "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"

"""Decompress the archive"""

!tar -xf  /content/sample_data/aclImdb_v1.tar.gz -C /content/sample_data/

"""Check that the folder aclImdb exists"""

!ls /content/sample_data/

"""Import all required libraries"""

import pandas as pd
import numpy as np
import os
from bs4 import BeautifulSoup
from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
import nltk
from gensim.models import Word2Vec, Phrases

import tensorflow as tf
from keras.utils import pad_sequences

"""Define function that reads all the files from a given folder"""

def read_data(path, files):
  data = []
  for f in files:
    with open(path+f) as file:
      ### BEGIN YOUR CODE HERE
      ## Read a line from the file and append it to the data list variable.
      ## TIP: use function append(): https://www.w3schools.com/python/ref_list_append.asp
      ## and readline(): https://www.w3schools.com/python/ref_file_readline.asp

      ### END YOUR CODE HERE
  return data

"""Load data (movie reviews), both for train and test"""

path_data_train_pos = path_data + '/aclImdb/train/pos/'
path_data_train_neg = path_data + '/aclImdb/train/neg/'
path_data_test_pos = path_data + '/aclImdb/test/pos/'
path_data_test_neg = path_data + '/aclImdb/test/neg/'

# Get the list of files from the four folders
train_pos_files = os.listdir(path_data_train_pos)
train_neg_files = os.listdir(path_data_train_neg)
test_pos_files = os.listdir(path_data_test_pos)
test_neg_files = os.listdir(path_data_test_neg)

### BEGIN YOUR CODE HERE
## Read the review data in these four variables: train_data_pos, train_data_neg,
## test_data_pos and test_data_neg using the function defined above read_data().
## Tip: First argument is the folder containing the files, second argument is
# the list of files


# Print examples of positive and negative reviews from training and testing dataset
# Tip: the output of the read_data() function is a list.
# https://www.w3schools.com/python/python_lists_access.asp

### END YOUR CODE HERE

# Let's work on a subset of training and testing dataset to start with.
sample_number = 1000
train_data_pos = train_data_pos[:sample_number]
train_data_neg = train_data_neg[:sample_number]
test_data_pos = test_data_pos[:sample_number]
test_data_neg = test_data_neg[:sample_number]

"""Create the data structures that we'll use for training and testing"""

### BEGIN YOUR CODE HERE
# Assign the length of the train_data_pos to variable length_train_pos
# Tip: to get the length of an array, you can use the function len()

# Assign the length of the train_data_neg to variable length_train_neg

# Assign the length of the test_data_pos to variable length_test_pos

# Assign the length of the test_data_neg to variable length_test_neg

### END YOUR CODE HERE

print("Length of the positive training examples is", length_train_pos )
print("Length of the negative training examples is", length_train_neg)
print("Length of the positive testing examples is", length_test_pos )
print("Length of the negative testing examples is", length_test_neg)

data_train = pd.DataFrame(zip(train_data_pos+train_data_neg, [1]*length_train_pos+[0]*length_train_neg))
data_test = pd.DataFrame(zip(test_data_pos+test_data_neg, [1]*length_test_pos+[0]*length_test_pos))

all_reviews = train_data_pos+train_data_neg+test_data_pos+test_data_neg

print("The length of the train reviews is",len(data_train))
print("The length of the test reviews is",len(data_test))
print("The length of all reviews is",len(all_reviews))

"""## The following are functions for preprocessing text"""

# Download stopwords and wordnet vectors
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

counter = 0
REPLACE_WITH_SPACE = re.compile(r'[^A-Za-z\s]')
stop_words = set(stopwords.words("english"))
# Declare the lemmatizer
lemmatizer = WordNetLemmatizer()

# The following functions preprocess text data
def clean_review(raw_review: str) -> str:
    # 1. Remove HTML
    review_text = BeautifulSoup(raw_review, "lxml").get_text()
    # 2. Remove non-letters
    letters_only = REPLACE_WITH_SPACE.sub(" ", review_text)
    # 3. Convert to lower case
    lowercase_letters = letters_only.lower()
    return lowercase_letters


def lemmatize(tokens: list) -> list:
    # 1. Lemmatize
    tokens = list(map(lemmatizer.lemmatize, tokens))
    lemmatized_tokens = list(map(lambda x: lemmatizer.lemmatize(x, "v"), tokens))
    # 2. Remove stop words
    meaningful_words = list(filter(lambda x: not x in stop_words, lemmatized_tokens))
    return meaningful_words


def preprocess(review: str, total: int, show_progress: bool = True) -> list:
    if show_progress:
        global counter
        counter += 1
        print('Processing... %6i/%6i'% (counter, total), end='\r')
    # 1. Clean text
    review = clean_review(review)
    # 2. Split into individual words
    tokens = word_tokenize(review)
    # 3. Lemmatize
    lemmas = lemmatize(tokens)
    # 4. Join the words back into one string separated by space,
    # and return the result.
    return lemmas

"""## Preprocess the text of the reviews by removing the non-word characters, converting everything to lower case, and lemmatizing words

"""

### Begin your code here
# Print the first review in the training data. This is in order to see
# how the data looks before processing.
# Tip: The training data is in a pandas data frame variable defined earlier.
# You can think of the data frame as an n-dimensional array. In this particular
# case, it is a 2-dimensional array.

# Print the label of the firest review of the training data

# Print the preprocess text of the first review in the training data
# Tip: Use the function preprocess() defined above. The first argument is the
# first review of the training data, and the second argument of the preprocess
# function should be 1, because we want to preprocess only one example


### End your code here

"""Let's preproces the entire set of reviews"""

all_reviews = np.array(list(map(lambda x: preprocess(x, len(all_reviews)), all_reviews)))

X_train_preprocessed = all_reviews[:(len(train_data_pos)+len(train_data_neg))]

# Let's see how many reviews we have in total for training
# We should get 2000 if you kept the sample_number=1000
print(X_train_preprocessed.shape)

"""## Compute Word2Vec vectors on all reviews"""

# compute bigrams, meaning detect phrases in the texts
# For example: ["new","york"] will be detected as one phrase "new york"
print("Compute phrases begin")
bigrams = Phrases(sentences=all_reviews)
# compute trigrams, meaning we detect three words that usually appear
# together. Notice that because we work with a small subset, we might
# not detect a lot of trigrams
trigrams = Phrases(sentences=bigrams[all_reviews])
print("Compute phrases end")

# Test how our phrase looks after calling the bigrams
print(bigrams['space station near the solar system'.split()])

# Test how our phrase looks after calling the trigrams
# Do you notice any difference compared with the bigrams?
print(trigrams[bigrams['space station near the solar system'.split()]])

# compute word embedding from the dataset
### Begin your code here
# set the embedding vector size variable to 256


### End your code here

# Next, we train a custom word2vec model based on our custom dataset.
# Notice that the input sentences are the trigrams
# In this case, we consider grouping like new_york one word.
# The input of the word2vec will be the processed words as trigrams.
# The duration of this process depends on the size of the dataset.
# For the restricted size of all reviews, this would take around 1-2minutes
print("Start learning the word embedding")
trigram_model = Word2Vec(
    sentences = trigrams[bigrams[all_reviews]],
    vector_size = embedding_vector_size,
    min_count=3, window=5, workers=4)
print("Done learning")

"""Check what is the vocabulary size"""

print("Vocabulary size:", len(trigram_model.wv))

"""Let's check the most similar words for "movie" & "galaxy"
"""

trigram_model.wv.most_similar('sun')
# If you are working with the subset of 1000 reviews, the most similar words might not be
# the most relevant ones. You can remove the constraint of working with only 1000 reviews,
# and compare what are the most similar words again, but please be aware that this might
# increase the training time of the word2vec

trigram_model.wv.most_similar('action')

"""Given a list of words identify which word does not match with the others"""

trigram_model.wv.doesnt_match(['moon', 'sun', 'planet'])

"""# Transform our reviews from the training set into vectors"""

def vectorize_data(data, vocab: dict) -> list:
    print('Vectorize sentences...', end='\r')
    keys = list(vocab.keys())
    filter_unknown = lambda word: vocab.get(word, None) is not None
    encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))
    vectorized = list(map(encode, data))
    print('Vectorize sentences... (done)')
    return vectorized

print('Convert sentences to sentences with ngrams...', end='\r')
X_data = trigrams[bigrams[X_train_preprocessed]]
print('Convert sentences to sentences with ngrams... (done)')
input_length = 150

# Transform all sequences to 150, sequences shorter are padded, while sequences longer are truncated to maximum size
X_pad = pad_sequences(
    sequences=vectorize_data(X_data, vocab=trigram_model.wv.key_to_index),
    maxlen=input_length,
    padding='post')
print('Transform sentences to sequences... (done)')

# For a given example, each number in the vector represents the position of the word in the vocabulary
X_pad[2]

"""# Train a classifier based on a particular type of recurrent neural network called LSTM to differentiate between positive and negative reviews"""

from sklearn.model_selection import train_test_split

#We'll train a model based on a subset of the training set
X_train, X_test, y_train, y_test = train_test_split(
    X_pad,
    data_train[1],
    test_size=0.05,
    shuffle=True,
    random_state=42)

data_train[1]

"""Define the Neural Network"""

### Begin your code here
# Define a neural network model
# TIP: Use the same sequential model in order to define the network:
# https://www.tensorflow.org/guide/keras/sequential_model
# You can also see an example in the MNIST lab.
# Add the following layers:
# 1. an Embedding layer of the following form:
# tf.keras.layers.Embedding(
#         input_dim = trigram_model.wv.vectors.shape[0],
#         output_dim = trigram_model.wv.vectors.shape[1],
#         input_length = input_length,
#         weights = [trigram_model.wv.vectors],
#         trainable=False)
# 2. A Bidirectional layer with LSTM, with 128 internal units and a recurrent dropout of 0.1
# A sentence can be considered a temporal sequence, where the order of the words
# might be important. This is why we need a temporal model, like a Long short-term memory model.
# A bidirectional model, simply means that the we want to parse the data both forward
# and backwards. This allows the network to capture both past and future context for each time step.
# See example here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional
# eg: tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, recurrent_dropout=0.1)),
# 3. A dropout layer with 0.25 probability
# You will find an example of dropout layer in the previous MNIST lab.
# See example here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout
# 4. A Dense layer with 64 internal units
# You will find an example of a dense layer in the previous MNIST lab.
# 5. A dropout layer with 0.3 probability
# 6. A final dense layer with 1 neuron and a sigmoid activation function
# tf.keras.layers.Dense(1, activation='sigmoid')


### End your code here

# compile the model
model.compile(loss='binary_crossentropy',
              optimizer=tf.keras.optimizers.Adam(),
              metrics=['accuracy'])

### Begin your code here
# Train the model with two epochs, and a batch size of 100.
# Tip: The x is X_train, y is y_train, and validation_data is (X_test, y_test)
# To view the model.fit function definition check here: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit
# and also an example here: https://www.tensorflow.org/guide/keras/training_with_built_in_methods (Look for fit() )
# To obtain a better accuracy you would need to train for more epochs. Find a right balance between the training time
# and the accuracy
# The parameters that you need to set are:
# x as X_train
# y as y_train
# validation_data with the (X_test, y_test)
# batch_size to be 100
# epochs to be 1


### End your code here

### Begin your code here
# Evaluate the trained model on the test set. How does the testing accuracy compare
# with the training one?
# Tip: use model.evaluate
# the x would be the X_test and y is y_test


### End your code here

# What is the accuracy you get?
# If you get an accuracy of aprox 50%, what does it mean? Did your model learn?
# Try to modify the architecture; how you train the model or how much data you
# use to train it in order to improve the results.

"""Test the model with a random text

"""

test_samples = []

# normally in python we have one line per instruction. Defining a long string
# will be difficult to read if it is only in one line. The way that we tell
# the interpreter that we have an instruction that spans several lines
# is by using the character \
review1 = "Petter Mattei's 'Love in the Time of Money' is a visually stunning"\
          "film to watch. Mr. Mattei offers us a vivid portrait about human" \
          " This is a movie that seems to be telling us what money, power and" \
          "success do to people"
### Begin your code here
# Write a couple of reviews and analyse how the model performs on your own data.
# Are the results what you expect?
# What did you change in the network architecture to get better results?
review2 = ""
review3 = ""
### End your code here

test_samples.append(review1)
test_samples.append(review2)
test_samples.append(review3)

test_samples_preprocess = np.array(list(map(lambda x: preprocess(x, len(test_samples)), test_samples)))
print(test_samples_preprocess)
print(trigrams[bigrams[test_samples_preprocess]])
test_data_bigrams = trigrams[bigrams[test_samples_preprocess]]
test_data_pad = pad_sequences(
sequences=vectorize_data(test_data_bigrams, vocab=trigram_model.wv.key_to_index),
maxlen=input_length,
padding='post')

predictions = model.predict(test_data_pad)
#print(predictions)

for (t, p) in zip( test_samples, predictions):
    prediction_string = "positive"
    if p<0.5:
        prediction_string = "negative"
    print("Predicted "+prediction_string+" "+str(p)+" for review:"+ t)